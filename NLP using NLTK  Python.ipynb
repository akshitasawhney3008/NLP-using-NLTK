{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process by which big quantity of text is divided into smaller parts called tokens.\n",
    "\n",
    "Natural Language toolkit has very important module tokenize which further comprises of sub-modules\n",
    "\n",
    "    **word tokenize**\n",
    "    **sentence tokenize**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of Words\n",
    "We use the method **word_tokenize()** to split a sentence into words. The output of word tokenization can be converted to Data Frame for better text understanding in machine learning applications. It can also be provided as input for further text cleaning steps such as punctuation removal, numeric character removal or stemming. Machine learning models need numeric data to be trained and make a prediction. Word tokenization becomes a crucial part of the text (string) to numeric data conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\IT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To start using NLTK in Jupyter we have to download punkt: Punkt Sentence Tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(word_tokenize(text))\n",
    "#Text variable is passed in word_tokenize module and printed the result. This module breaks each word with punctuation which you can see in the output.\n",
    "#Tokenization of Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of Sentences\n",
    "Sub-module available for the above is sent_tokenize. \n",
    "For accomplishing a task of counting average words per sentence, you need both sentence tokenization as well as words to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God is Great!', 'I won a lottery.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(sent_tokenize(text))\n",
    "#Further sent module parsed that sentences and show output. It is clear that this function breaks each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have 12 words and two sentences for the same input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS (Part-Of-Speech) Tagging & Chunking with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "Parts of speech Tagging is responsible for reading the text in a language and assigning some specific token (Parts of Speech) to each word.\n",
    "\n",
    "    Input: Everything to permit us.\n",
    "\n",
    "    Output: [('Everything', NN),('to', TO), ('permit', VB), ('us', PRP)]\n",
    "    \n",
    "Every token above has a meaning that can be looked for from the table available in NLTK. \n",
    "For example above:\n",
    "\n",
    "    NN means noun, \n",
    "    TO is the infinite marker(to), \n",
    "    VB means verb \n",
    "    PRP means personal pronoun (hers, herself, him,himself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\IT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'python', 'from', 'GL', 'and', 'make', 'study', 'easy']\n",
      "After Token: [('learn', 'JJ'), ('python', 'NN'), ('from', 'IN'), ('GL', 'NNP'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"learn python from GL and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "Chunking is used to add more structure to the sentence by following parts of speech (POS) tagging. It is also known as shallow parsing. The resulted group of words is called \"chunks.\"\n",
    "\n",
    "The primary usage of chunking is to make a group of \"noun phrases.\" The parts of speech are combined with regular expressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you need to tag Noun, verb (past tense), adjective, and coordinating junction from the sentence. You can use the rule as below:\n",
    "\n",
    "    chunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\n",
    "here:\n",
    "\n",
    "    \".\" means Any character except new line\n",
    "    \"*\" means Match 0 or more repetitions\n",
    "    \"?\" means Match 0 or 1 repetitions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk learn/JJ)\n",
      "  (mychunk python/NN)\n",
      "  from/IN\n",
      "  (mychunk GL/NNP and/CC)\n",
      "  make/VB\n",
      "  (mychunk study/NN easy/JJ))\n"
     ]
    }
   ],
   "source": [
    "# EARLIER OUTPUT:\n",
    "#[('learn', 'JJ'), ('python', 'NN'), ('from', 'IN'), ('greatlearning', 'NN'), \n",
    "#('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conclusion from the above example: \"make\" is a verb which is not included in the rule, so it is not tagged as mychunk.\n",
    "#If we convert make ro made it will be added in our my chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.draw() \n",
    "# It will draw the pattern graphically which can be seen in Noun Phrase chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization with Python NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.\n",
    "\n",
    "For example, the root word is \"eat\" and it's variations are \"eats, eating, eaten and like so\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: \n",
    "\n",
    "    He was riding.\t\n",
    "    He was taking the ride.\n",
    "In the above two sentences, the meaning is the same.But for machines, both sentences are different. Thus it became hard to convert it into the same data row. \n",
    "\n",
    "In case we do not provide the same data-set, then machine fails to predict. So it is necessary to differentiate the meaning of each word to prepare the dataset for machine learning. And here stemming is used to categorize the same type of data by getting its root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps =PorterStemmer()  # An object is created which belongs to class nltk.stem.porter.PorterStemmer.\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be concluded that stemming is considered as an important preprocessing step because it removed redundancy in the data and variations in the same word. As a result, data is filtered which will help in better machine training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "glguru\n",
      ",\n",
      "you\n",
      "have\n",
      "to\n",
      "build\n",
      "a\n",
      "veri\n",
      "good\n",
      "site\n",
      "and\n",
      "I\n",
      "love\n",
      "visit\n",
      "your\n",
      "site\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentence=\"Hello GLguru, You have to build a very good site and I love visiting your site.\"\n",
    "words = word_tokenize(sentence)\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a data-preprocessing module. The English language has many variations of a single word. These variations create ambiguity in machine learning training and prediction. To create a successful model, it's vital to filter such words and convert to the same type of sequenced data using stemming. Also, this is an important technique to get row data from a set of sentence and removal of redundant data also known as normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. It helps in returning the base or dictionary form of a word, which is known as the lemma.\n",
    "Text preprocessing includes both stemming as well as lemmatization. There is a difference between these both. Lemmatization is preferred over the former because of the below reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between stemming an Lemmatization\n",
    "Stemming algorithm works by cutting the suffix from the word. In a broader sense cuts either the beginning or end of the word.\n",
    "\n",
    "On the contrary, Lemmatization is a more powerful operation, and it takes into consideration morphological analysis of the words. It returns the lemma which is the base form of all its inflectional forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for studies is studi\n",
      "Stemming for studying is studi\n",
      "Stemming for cries is cri\n",
      "Stemming for cry is cri\n"
     ]
    }
   ],
   "source": [
    "#Stemming code\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer  = PorterStemmer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\IT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization code\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look stemming for studies and studying, output is same (studi) but lemmatizer provides different lemma for both tokens study for studies and studying for studying. So when we need to make feature set to train machine, it would be great if lemmatization is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\IT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english'))\n",
    "words = word_tokenize(data)\n",
    "wordsFiltered = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "Bag of Words (BOW) is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a high level BOW involves following words:\n",
    "\n",
    "    Clean Text\n",
    "    Tokenize\n",
    "    Build Vocab \n",
    "    Generate vectors\n",
    "Generated vectors can be input to your machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Paragraph\n",
    "text = \"\"\"Beans. I was trying to explain to somebody as we were flying in, that’s corn. That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn’t speak at the commencement.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beans. I was trying to explain to somebody as we were flying in, that’s corn. That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn’t speak at the commencement.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #1 : We will first preprocess the data, in order to:\n",
    "\n",
    "    Convert text to lower case.\n",
    "    Remove all non-word characters.\n",
    "    Remove all punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "  \n",
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)): \n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])    #remove non-word\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])   # remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beans ',\n",
       " 'i was trying to explain to somebody as we were flying in that s corn ',\n",
       " 'that s beans ',\n",
       " 'and they were very impressed at my agricultural knowledge ',\n",
       " 'please give it up for amaury once again for that outstanding introduction ',\n",
       " 'i have a bunch of good friends here today including somebody who i served with who is one of the finest senators in the country and we re lucky to have him your senator dick durbin is here ',\n",
       " 'i also noticed by the way former governor edgar here who i haven t seen in a long time and somehow he has not aged and i have ',\n",
       " 'and it s great to see you governor ',\n",
       " 'i want to thank president killeen and everybody at the u of i system for making it possible for me to be here today ',\n",
       " 'and i am deeply honored at the paul douglas award that is being given to me ',\n",
       " 'he is somebody who set the path for so much outstanding public service here in illinois ',\n",
       " 'now i want to start by addressing the elephant in the room ',\n",
       " 'i know people are still wondering why i didn t speak at the commencement ']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #2 : Obtaining most frequent words in our text.\n",
    "\n",
    "We will apply the following steps to generate our model.\n",
    "\n",
    "    We declare a dictionary to hold our bag of words.\n",
    "    Next we tokenize each sentence to words.\n",
    "    Now for each word in sentence, we check if the word exists in our dictionary.\n",
    "    If it does, then we increment its count by 1. If it doesn’t, we add it to our dictionary and set its count as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model \n",
    "word2count = {} \n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beans': 2,\n",
       " 'i': 12,\n",
       " 'was': 1,\n",
       " 'trying': 1,\n",
       " 'to': 8,\n",
       " 'explain': 1,\n",
       " 'somebody': 3,\n",
       " 'as': 1,\n",
       " 'we': 2,\n",
       " 'were': 2,\n",
       " 'flying': 1,\n",
       " 'in': 5,\n",
       " 'that': 4,\n",
       " 's': 3,\n",
       " 'corn': 1,\n",
       " 'and': 7,\n",
       " 'they': 1,\n",
       " 'very': 1,\n",
       " 'impressed': 1,\n",
       " 'at': 4,\n",
       " 'my': 1,\n",
       " 'agricultural': 1,\n",
       " 'knowledge': 1,\n",
       " 'please': 1,\n",
       " 'give': 1,\n",
       " 'it': 3,\n",
       " 'up': 1,\n",
       " 'for': 5,\n",
       " 'amaury': 1,\n",
       " 'once': 1,\n",
       " 'again': 1,\n",
       " 'outstanding': 2,\n",
       " 'introduction': 1,\n",
       " 'have': 3,\n",
       " 'a': 2,\n",
       " 'bunch': 1,\n",
       " 'of': 3,\n",
       " 'good': 1,\n",
       " 'friends': 1,\n",
       " 'here': 5,\n",
       " 'today': 2,\n",
       " 'including': 1,\n",
       " 'who': 4,\n",
       " 'served': 1,\n",
       " 'with': 1,\n",
       " 'is': 4,\n",
       " 'one': 1,\n",
       " 'the': 9,\n",
       " 'finest': 1,\n",
       " 'senators': 1,\n",
       " 'country': 1,\n",
       " 're': 1,\n",
       " 'lucky': 1,\n",
       " 'him': 1,\n",
       " 'your': 1,\n",
       " 'senator': 1,\n",
       " 'dick': 1,\n",
       " 'durbin': 1,\n",
       " 'also': 1,\n",
       " 'noticed': 1,\n",
       " 'by': 2,\n",
       " 'way': 1,\n",
       " 'former': 1,\n",
       " 'governor': 2,\n",
       " 'edgar': 1,\n",
       " 'haven': 1,\n",
       " 't': 2,\n",
       " 'seen': 1,\n",
       " 'long': 1,\n",
       " 'time': 1,\n",
       " 'somehow': 1,\n",
       " 'he': 2,\n",
       " 'has': 1,\n",
       " 'not': 1,\n",
       " 'aged': 1,\n",
       " 'great': 1,\n",
       " 'see': 1,\n",
       " 'you': 1,\n",
       " 'want': 2,\n",
       " 'thank': 1,\n",
       " 'president': 1,\n",
       " 'killeen': 1,\n",
       " 'everybody': 1,\n",
       " 'u': 1,\n",
       " 'system': 1,\n",
       " 'making': 1,\n",
       " 'possible': 1,\n",
       " 'me': 2,\n",
       " 'be': 1,\n",
       " 'am': 1,\n",
       " 'deeply': 1,\n",
       " 'honored': 1,\n",
       " 'paul': 1,\n",
       " 'douglas': 1,\n",
       " 'award': 1,\n",
       " 'being': 1,\n",
       " 'given': 1,\n",
       " 'set': 1,\n",
       " 'path': 1,\n",
       " 'so': 1,\n",
       " 'much': 1,\n",
       " 'public': 1,\n",
       " 'service': 1,\n",
       " 'illinois': 1,\n",
       " 'now': 1,\n",
       " 'start': 1,\n",
       " 'addressing': 1,\n",
       " 'elephant': 1,\n",
       " 'room': 1,\n",
       " 'know': 1,\n",
       " 'people': 1,\n",
       " 'are': 1,\n",
       " 'still': 1,\n",
       " 'wondering': 1,\n",
       " 'why': 1,\n",
       " 'didn': 1,\n",
       " 'speak': 1,\n",
       " 'commencement': 1}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model, we have a total of 118 words. However when processing large texts, the number of words could reach millions. We do not need to use all those words. Hence, we select a particular number of most frequently used words. To implement this we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get) \n",
    "#where 100 denotes the number of words we want. If our text is large, we feed in a larger number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step #3 : Building the Bag of Words model\n",
    "\n",
    "        In this step we construct a vector, which would tell us whether a word in each sentence is a frequent word or not. \n",
    "        If a word in a sentence is a frequent word, we set it as 1, else we set it as 0.\n",
    "This can be implemented with the help of following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] \n",
    "for data in dataset: \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in nltk.word_tokenize(data): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
